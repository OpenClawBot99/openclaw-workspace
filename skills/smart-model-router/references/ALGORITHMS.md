# 智能模型路由算法详解

本文档详细解释智能模型路由器中使用的三种强化学习策略。

---

## 问题定义

**Multi-Armed Bandit (MAB) 问题**

- 有 K 个模型（老虎机的臂）
- 每次选择一个模型进行对话
- 获得奖励（用户满意度、速度、成本等）
- 目标：最大化累积奖励

模型选择的核心矛盾：
- **利用 (Exploit)**：选择当前表现最好的模型
- **探索 (Explore)**：尝试未充分使用的模型，发现更好的选择

---

## 1. ε-Greedy (Epsilon-Greedy)

### 原理

每次选择时：
- 以概率 ε 随机选择一个模型（探索）
- 以概率 1-ε 选择当前评分最高的模型（利用）

### 公式

```
ε = max(ε_min, ε_start - decay × total_uses)

if random() < ε:
    选择 随机模型
else:
    选择 score 最大的模型
```

### 参数含义

| 参数 | 默认值 | 说明 |
|------|----------|------|
| ε_start | 0.5 | 初始探索率，早期更多探索 |
| ε_min | 0.1 | 最小探索率，保持一定探索性 |
| decay | 0.001 | 每次使用后的衰减量 |

### 优点

- **简单高效**：计算复杂度 O(1)
- **可解释性强**：直观的探索-利用权衡
- **适合冷启动**：早期充分探索

### 缺点

- **探索不够聪明**：随机探索可能浪费在明显不好的模型上
- **ε 难以调优**：固定衰减可能不适合所有场景

### 适用场景

- 数据量少（< 20 次使用）
- 需要快速了解各模型特性
- 模型差异较大

---

## 2. Softmax (Boltzmann Distribution)

### 原理

基于模型评分的概率分布选择，评分高的模型被选中概率更大，但低分模型也有机会。

### 公式

```
τ = max(τ_min, τ_start - decay × total_uses)

P(model_i) = exp(score_i / τ) / Σ exp(score_j / τ)
```

选择模型 i 的概率 = e^(score_i/τ) 除以所有模型的 e^(score_j/τ) 之和

### 参数含义

| 参数 | 默认值 | 说明 |
|------|----------|------|
| τ_start | 1.0 | 初始温度，高温度 = 更随机 |
| τ_min | 0.5 | 最小温度，保持一定随机性 |
| decay | 0.002 | 温度衰减速度 |

### 温度的影响

- **τ 高（如 1.0）**：概率分布均匀，接近随机探索
- **τ 低（如 0.5）**：高分模型概率远高于低分模型，接近贪婪

### 优点

- **更智能的探索**：基于评分选择，而非完全随机
- **平滑过渡**：逐渐从探索转向利用
- **灵活性**：温度参数可调节探索强度

### 缺点

- **计算稍复杂**：需要计算指数和概率
- **温度调优敏感**：τ 的选择影响较大

### 适用场景

- 数据量中等（20-100 次使用）
- 需要平衡探索和利用
- 模型性能差异不明显

---

## 3. UCB (Upper Confidence Bound)

### 原理

选择「置信区间上界」最大的模型，平衡均值（利用）和不确定性（探索）。

### 公式

```
UCB_i = μ_i + c × √(ln(N) / n_i)

其中：
- μ_i: 模型 i 的平均评分
- n_i: 模型 i 的使用次数
- N: 总使用次数
- c: 探索系数（默认 2.0）
```

选择 `UCB` 值最大的模型。

### 参数含义

| 参数 | 默认值 | 说明 |
|------|----------|------|
| c | 2.0 | 探索系数，越大越偏向探索 |

### 置信区间解释

- **均值 μ_i**：模型的平均表现（利用）
- **不确定性 √(ln(N)/n_i)**：使用次数越少，不确定性越高（探索）

### 优点

- **理论保证**：有最优性的理论保证
- **渐进最优**：长期收敛到最优策略
- **不确定性感知**：自动探索未充分使用的模型

### 缺点

- **计算稍复杂**：需要计算对数和平方根
- **冷启动慢**：需要一定数据量才能显示优势

### 适用场景

- 数据量充足（> 100 次使用）
- 追求长期最优性能
- 需要理论保证

---

## 综合评分计算

所有策略都基于相同的评分体系：

### 指标归一化

| 指标 | 归一化方式 | 目标 |
|--------|-------------|------|
| 用户满意度 | satisfaction / 5.0 | 1-5 → 0-1 |
| 响应速度 | min(1.0, 3000 / speed) | 越快越好，3000ms 为参考 |
| 成本 | min(1.0, 0.1 / cost) | 越便宜越好，$0.1 为参考 |
| 错误率 | 1.0 - error_rate | 越低越好 |

### 加权求和

```
score = 0.4 × satisfaction_norm +
        0.2 × speed_norm +
        0.2 × cost_norm +
        0.2 × error_norm
```

最终评分范围：0-5 分

---

## 参数调优建议

### ε-Greedy

- **快速探索**：ε_start = 0.7, decay = 0.002
- **保守策略**：ε_start = 0.3, decay = 0.0005

### Softmax

- **快速收敛**：τ_start = 0.8, decay = 0.003
- **持续探索**：τ_start = 1.2, decay = 0.001

### UCB

- **激进探索**：c = 3.0
- **保守利用**：c = 1.0

---

## 自动策略选择逻辑

智能路由器根据总使用次数自动选择策略：

```
总使用次数 < 20   → ε-Greedy  （快速探索）
20 ≤ 次数 < 100   → Softmax     （平衡）
次数 ≥ 100         → UCB         （追求最优）
```

---

## 数学推导参考

- ε-Greedy: Sutton & Barto, "Reinforcement Learning: An Introduction"
- Softmax: Boltzmann distribution in statistical mechanics
- UCB: Auer et al., "Finite-time Analysis of the Multiarmed Bandit Problem"

---

## 总结

| 策略 | 探索智能度 | 计算复杂度 | 理论保证 | 适用阶段 |
|--------|-------------|-------------|-----------|---------|
| ε-Greedy | 低 | O(1) | 无 | 早期 |
| Softmax | 中 | O(K) | 无 | 中期 |
| UCB | 高 | O(K) | 有 | 后期 |

选择策略的关键：
1. **数据量**：少 → 多探索，多 → 利用
2. **模型差异**：大 → 随机探索，小 → 智能探索
3. **长期目标**：短期 → 快速探索，长期 → 渐进最优
