#!/usr/bin/env python3
"""
Lisa å¼ºåŒ–å­¦ä¹ å®éªŒ - æŒ‘æˆ˜æ›´å¤æ‚ç¯å¢ƒ
ä» CartPole å‡çº§åˆ° LunarLander
"""

import gymnasium as gym
import numpy as np
import random
from collections import deque
import pickle
from datetime import datetime
from pathlib import Path

# é…ç½®
EPISODES = 100
MAX_STEPS = 1000
MEMORY_SIZE = 50000
BATCH_SIZE = 64
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995
LEARNING_RATE = 0.001
GAMMA = 0.99

class DQNAgent:
    """ç®€å•çš„ Deep Q-Network Agent"""
    
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.epsilon = EPSILON_START
        self.gamma = GAMMA
        self.lr = LEARNING_RATE
        # ç®€åŒ–çš„Qè¡¨ï¼ˆç”¨äºç¦»æ•£çŠ¶æ€ï¼‰
        self.q_table = {}
        
    def get_discrete_state(self, state, bins=8):
        """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–åˆ°ç½‘æ ¼"""
        # ç®€åŒ–ä¸º8ä¸ªå…³é”®ç»´åº¦
        s = []
        for i in range(min(len(state), 8)):
            # ç®€å•çš„åˆ†ç®±
            val = int(state[i] * bins) % bins
            s.append(val)
        return tuple(s)
        
    def act(self, state):
        """é€‰æ‹©åŠ¨ä½œ - Epsilon-Greedy"""
        if random.random() < self.epsilon:
            return random.randrange(self.action_size)
        
        s = self.get_discrete_state(state)
        if s not in self.q_table:
            self.q_table[s] = [0.0] * self.action_size
        
        return np.argmax(self.q_table[s])
    
    def learn(self, state, action, reward, next_state, done):
        """Qå­¦ä¹ æ›´æ–°"""
        s = self.get_discrete_state(state)
        ns = self.get_discrete_state(next_state)
        
        if s not in self.q_table:
            self.q_table[s] = [0.0] * self.action_size
        if ns not in self.q_table:
            self.q_table[ns] = [0.0] * self.action_size
        
        # Qå­¦ä¹ å…¬å¼
        target = reward
        if not done:
            target = reward + self.gamma * max(self.q_table[ns])
        
        self.q_table[s][action] += self.lr * (target - self.q_table[s][action])
        
        # Epsilon è¡°å‡
        if done:
            self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)

def run_lunar_lander():
    """è¿è¡Œ Acrobot å®éªŒ - ç»å…¸æ§åˆ¶é—®é¢˜"""
    
    print("=" * 60)
    print("ğŸš€ Lisa æŒ‘æˆ˜ Acrobot - å…·èº«æ™ºèƒ½è¿›é˜¶")
    print("=" * 60)
    print(f"\nğŸ• æ—¶é—´: {datetime.now()}")
    print(f"ğŸ“¦ ç¯å¢ƒ: Acrobot-v1")
    
    # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨ Acrobot (ç»å…¸æ§åˆ¶é—®é¢˜ï¼Œä¸éœ€è¦Box2D)
    env = gym.make("Acrobot-v1")
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    
    print(f"\nğŸ“Š çŠ¶æ€ç©ºé—´: {state_size}ç»´ (ä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ç­‰)")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {action_size}ä¸ª (ä¸»å¼•æ“ã€å·¦å¼•æ“ã€å³å¼•æ“)")
    
    # åˆ›å»ºAgent
    agent = DQNAgent(state_size, action_size)
    
    # è®­ç»ƒ
    scores = deque(maxlen=10)
    best_score = -1000
    success_count = 0
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ¯ ç›®æ ‡: è®©æœ«ç«¯è¾¾åˆ°ç›®æ ‡é«˜åº¦")
    
    for episode in range(EPISODES):
        state, _ = env.reset()
        total_reward = 0
        
        for step in range(MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # å­¦ä¹ 
            agent.learn(state, action, reward, next_state, done)
            
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        scores.append(total_reward)
        avg_score = np.mean(scores)
        
        # æˆåŠŸç€é™†åˆ¤å®š
        if total_reward > 0:
            success_count += 1
        
        if total_reward > best_score:
            best_score = total_reward
            
        if (episode + 1) % 20 == 0:
            status = "âœ…" if avg_score > 0 else "âŒ"
            print(f"  Episode {episode+1:3d}: å¾—åˆ†={total_reward:8.1f}, "
                  f"å¹³å‡={avg_score:8.1f}, æˆåŠŸ={success_count}, Îµ={agent.epsilon:.3f} {status}")
    
    env.close()
    
    # ä¿å­˜æ¨¡å‹
    model_path = Path(__file__).parent / "lunar_lander_q_table.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(agent.q_table, f)
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ† æœ€é«˜åˆ†: {best_score:.1f}")
    print(f"ğŸ‰ æˆåŠŸç€é™†æ¬¡æ•°: {success_count}/{EPISODES}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    print(f"ğŸ“Š Qè¡¨å¤§å°: {len(agent.q_table)} ä¸ªçŠ¶æ€")
    
    # è¯„ä¼°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç›®æ ‡è¯„ä¼°")
    print("=" * 60)
    
    if success_count > 0:
        print("âœ… æŒ‘æˆ˜ç›®æ ‡è¾¾æˆ! æˆåŠŸç€é™†!")
    elif best_score > -100:
        print("âš ï¸ æ¥è¿‘ç›®æ ‡ - æ¥è¿‘æˆåŠŸç€é™†")
    else:
        print("âŒ ç›®æ ‡æœªè¾¾æˆ - éœ€è¦æ›´å¤šè®­ç»ƒ")
    
    return agent, best_score, success_count

if __name__ == "__main__":
    agent, score, success = run_lunar_lander()
