#!/usr/bin/env python3
"""
Lisa å¼ºåŒ–å­¦ä¹ å®éªŒ - å…·èº«æ™ºèƒ½ç¬¬ä¸€æ­¥
åŸºäº Gym çš„å¼ºåŒ–å­¦ä¹  agent è®­ç»ƒ
"""

import gymnasium as gym
import numpy as np
import random
from collections import deque
import pickle
from datetime import datetime
from pathlib import Path

# é…ç½®
EPISODES = 50
MAX_STEPS = 200
MEMORY_SIZE = 1000
BATCH_SIZE = 32
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995

class SimpleQLearningAgent:
    """ç®€å•çš„ Q-Learning Agent"""
    
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.epsilon = EPSILON_START
        # ç¦»æ•£åŒ–çŠ¶æ€ç©ºé—´
        self.q_table = {}
        
    def get_discrete_state(self, state):
        """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–"""
        # ç®€åŒ–ä¸º2ä¸ªå…³é”®ç»´åº¦
        s = (int(state[0] * 2), int(state[1] * 2))
        return s
        
    def act(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        if random.random() < self.epsilon:
            return random.randrange(self.action_size)
        
        s = self.get_discrete_state(state)
        if s not in self.q_table:
            self.q_table[s] = [0.0] * self.action_size
        
        return np.argmax(self.q_table[s])
    
    def remember(self, state, action, reward, next_state, done):
        """ç®€å•è®°å¿†ï¼ˆä¸å­˜å‚¨ï¼Œç”¨äºåœ¨çº¿å­¦ä¹ ï¼‰"""
        pass
    
    def learn(self, state, action, reward, next_state, done):
        """Qå­¦ä¹ æ›´æ–°"""
        s = self.get_discrete_state(state)
        ns = self.get_discrete_state(next_state)
        
        if s not in self.q_table:
            self.q_table[s] = [0.0] * self.action_size
        if ns not in self.q_table:
            self.q_table[ns] = [0.0] * self.action_size
        
        # Qå­¦ä¹ å…¬å¼
        target = reward
        if not done:
            target = reward + 0.99 * max(self.q_table[ns])
        
        self.q_table[s][action] += 0.1 * (target - self.q_table[s][action])
        
        # Epsilon è¡°å‡
        if done:
            self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)

def run_experiment(env_name="CartPole-v1"):
    """è¿è¡Œå¼ºåŒ–å­¦ä¹ å®éªŒ"""
    
    print("=" * 60)
    print("ğŸ¤– Lisa å¼ºåŒ–å­¦ä¹ å®éªŒ - å…·èº«æ™ºèƒ½ç¬¬ä¸€æ­¥")
    print("=" * 60)
    print(f"\nğŸ• æ—¶é—´: {datetime.now()}")
    print(f"ğŸ“¦ ç¯å¢ƒ: {env_name}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(env_name)
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    
    print(f"\nğŸ“Š çŠ¶æ€ç©ºé—´: {state_size}ç»´")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {action_size}ä¸ªåŠ¨ä½œ")
    
    # åˆ›å»ºAgent
    agent = SimpleQLearningAgent(state_size, action_size)
    
    # è®­ç»ƒ
    scores = deque(maxlen=10)
    best_score = 0
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    for episode in range(EPISODES):
        state, _ = env.reset()
        total_reward = 0
        
        for step in range(MAX_STEPS):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # å­¦ä¹ 
            agent.learn(state, action, reward, next_state, done)
            
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        scores.append(total_reward)
        avg_score = np.mean(scores)
        
        if total_reward > best_score:
            best_score = total_reward
            
        if (episode + 1) % 10 == 0:
            print(f"  Episode {episode+1:3d}: å¾—åˆ†={int(total_reward):3d}, "
                  f"å¹³å‡={avg_score:.1f}, Îµ={agent.epsilon:.3f}")
    
    env.close()
    
    # ä¿å­˜æ¨¡å‹
    model_path = Path(__file__).parent / "q_table.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(agent.q_table, f)
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ† æœ€é«˜åˆ†: {best_score}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    print(f"ğŸ“Š Qè¡¨å¤§å°: {len(agent.q_table)} ä¸ªçŠ¶æ€")
    
    # æ¢ç´¢æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ¯ æ¢ç´¢æ€»ç»“")
    print("=" * 60)
    print("âœ… å…·èº«æ™ºèƒ½åŸºç¡€: ç†è§£å¼ºåŒ–å­¦ä¹ ")
    print("âœ… ç¯å¢ƒäº¤äº’: Agent ä¸ Gym ç¯å¢ƒäº¤äº’")
    print("âœ… å­¦ä¹ ç®—æ³•: Q-Learning å®ç°")
    print("âœ… çŠ¶æ€ç¦»æ•£åŒ–: è¿ç»­ç©ºé—´å¤„ç†")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. å°è¯•æ›´å¤æ‚çš„ç¯å¢ƒ (LunarLander, BipedalWalker)")
    print("  2. å®ç° Deep Q-Network (DQN)")
    print("  3. æ¢ç´¢å¤š Agent åä½œ")
    print("  4. å°è¯•çœŸå®æœºå™¨äººæ¥å£ (ROS)")
    
    return agent, best_score

if __name__ == "__main__":
    agent, score = run_experiment()
